{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sc2MG4zMZPEu",
        "outputId": "f262bee0-40f8-4b22-868f-c29e4daabd32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu121\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vgyXlB-sZPEw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.utils import train_test_split_edges"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4QbU6zZZPEw"
      },
      "source": [
        "# Tutorial 6  \n",
        "Graph AutoEncoders GAE &  \n",
        "Variational Graph Autoencoders VGAE    \n",
        "\n",
        "[paper](https://arxiv.org/pdf/1611.07308.pdf)  \n",
        "[code](https://github.com/rusty1s/pytorch_geometric/blob/master/examples/autoencoder.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyJHd9boZPEx"
      },
      "source": [
        "## Graph AutoEncoder GAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNCVZfJcZPEx"
      },
      "source": [
        "### Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KHENa0GsZPEx",
        "outputId": "3722dfa2-9792-4369-d6c3-519328f0deef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\n",
            "Processing...\n",
            "Done!\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "dataset = Planetoid(\"\\..\", \"CiteSeer\", transform=T.NormalizeFeatures())\n",
        "dataset.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "B4PwfCkTZPEy",
        "outputId": "10900f7c-2385-4ac2-937e-fc8304046e4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "data = dataset[0]\n",
        "data.train_mask = data.val_mask = data.test_mask = None\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yD9o7v-aZPEy",
        "outputId": "21836bc4-0c30-4a5a-b81e-9adbef504e82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ],
      "source": [
        "data = train_test_split_edges(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hVf61tGPZPEy",
        "outputId": "869d7dcf-ec93-4f2f-f1ce-e148625c90ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(x=[3327, 3703], y=[3327], val_pos_edge_index=[2, 227], test_pos_edge_index=[2, 455], train_pos_edge_index=[2, 7740], train_neg_adj_mask=[3327, 3327], val_neg_edge_index=[2, 227], test_neg_edge_index=[2, 455])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIgC4VIMZPEy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcdJG39sZPEy"
      },
      "source": [
        "### Define the Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "o6NIFmjGZPEz"
      },
      "outputs": [],
      "source": [
        "class GCNEncoder(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GCNEncoder, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 2 * out_channels, cached=True) # cached only for transductive learning\n",
        "        self.conv2 = GCNConv(2 * out_channels, out_channels, cached=True) # cached only for transductive learning\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        return self.conv2(x, edge_index)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab1ZP1VvZPEz"
      },
      "source": [
        "### Define the Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BcoBLm5MZPEz"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "n8HfS3mnZPEz"
      },
      "outputs": [],
      "source": [
        "# parameters\n",
        "out_channels = 2\n",
        "num_features = dataset.num_features\n",
        "epochs = 100\n",
        "\n",
        "# model\n",
        "model = GAE(GCNEncoder(num_features, out_channels))\n",
        "\n",
        "# move to GPU (if available)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "x = data.x.to(device)\n",
        "train_pos_edge_index = data.train_pos_edge_index.to(device)\n",
        "\n",
        "# inizialize the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H4ONZ8ZbYo66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9DBrQPjlYu8s",
        "outputId": "506f0b2e-d2d2-40cb-816f-fd3920f18e37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['T_destination',\n",
              " '__annotations__',\n",
              " '__call__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattr__',\n",
              " '__getattribute__',\n",
              " '__getstate__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__setstate__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_apply',\n",
              " '_backward_hooks',\n",
              " '_backward_pre_hooks',\n",
              " '_buffers',\n",
              " '_call_impl',\n",
              " '_compiled_call_impl',\n",
              " '_forward_hooks',\n",
              " '_forward_hooks_always_called',\n",
              " '_forward_hooks_with_kwargs',\n",
              " '_forward_pre_hooks',\n",
              " '_forward_pre_hooks_with_kwargs',\n",
              " '_get_backward_hooks',\n",
              " '_get_backward_pre_hooks',\n",
              " '_get_name',\n",
              " '_is_full_backward_hook',\n",
              " '_load_from_state_dict',\n",
              " '_load_state_dict_post_hooks',\n",
              " '_load_state_dict_pre_hooks',\n",
              " '_maybe_warn_non_full_backward_hook',\n",
              " '_modules',\n",
              " '_named_members',\n",
              " '_non_persistent_buffers_set',\n",
              " '_parameters',\n",
              " '_register_load_state_dict_pre_hook',\n",
              " '_register_state_dict_hook',\n",
              " '_replicate_for_data_parallel',\n",
              " '_save_to_state_dict',\n",
              " '_slow_forward',\n",
              " '_state_dict_hooks',\n",
              " '_state_dict_pre_hooks',\n",
              " '_version',\n",
              " '_wrapped_call_impl',\n",
              " 'add_module',\n",
              " 'apply',\n",
              " 'bfloat16',\n",
              " 'buffers',\n",
              " 'call_super_init',\n",
              " 'children',\n",
              " 'compile',\n",
              " 'cpu',\n",
              " 'cuda',\n",
              " 'decode',\n",
              " 'decoder',\n",
              " 'double',\n",
              " 'dump_patches',\n",
              " 'encode',\n",
              " 'encoder',\n",
              " 'eval',\n",
              " 'extra_repr',\n",
              " 'float',\n",
              " 'forward',\n",
              " 'get_buffer',\n",
              " 'get_extra_state',\n",
              " 'get_parameter',\n",
              " 'get_submodule',\n",
              " 'half',\n",
              " 'ipu',\n",
              " 'load_state_dict',\n",
              " 'modules',\n",
              " 'mtia',\n",
              " 'named_buffers',\n",
              " 'named_children',\n",
              " 'named_modules',\n",
              " 'named_parameters',\n",
              " 'parameters',\n",
              " 'recon_loss',\n",
              " 'register_backward_hook',\n",
              " 'register_buffer',\n",
              " 'register_forward_hook',\n",
              " 'register_forward_pre_hook',\n",
              " 'register_full_backward_hook',\n",
              " 'register_full_backward_pre_hook',\n",
              " 'register_load_state_dict_post_hook',\n",
              " 'register_load_state_dict_pre_hook',\n",
              " 'register_module',\n",
              " 'register_parameter',\n",
              " 'register_state_dict_post_hook',\n",
              " 'register_state_dict_pre_hook',\n",
              " 'requires_grad_',\n",
              " 'reset_parameters',\n",
              " 'set_extra_state',\n",
              " 'set_submodule',\n",
              " 'share_memory',\n",
              " 'state_dict',\n",
              " 'test',\n",
              " 'to',\n",
              " 'to_empty',\n",
              " 'train',\n",
              " 'training',\n",
              " 'type',\n",
              " 'xpu',\n",
              " 'zero_grad']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "\n",
        "# model\n",
        "dir(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VUvh15XDZPEz"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    z = model.encode(x, train_pos_edge_index)\n",
        "    loss = model.recon_loss(z, train_pos_edge_index)\n",
        "    #if args.variational:\n",
        "    #   loss = loss + (1 / data.num_nodes) * model.kl_loss()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return float(loss)\n",
        "\n",
        "\n",
        "def test(pos_edge_index, neg_edge_index):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        z = model.encode(x, train_pos_edge_index)\n",
        "    return model.test(z, pos_edge_index, neg_edge_index)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "scrolled": true,
        "id": "-GxHWKWEZPEz",
        "outputId": "03c77f04-e4b8-4807-a5f6-648b44f428ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, AUC: 0.6281, AP: 0.6731\n",
            "Epoch: 002, AUC: 0.6420, AP: 0.6830\n",
            "Epoch: 003, AUC: 0.6462, AP: 0.6873\n",
            "Epoch: 004, AUC: 0.6475, AP: 0.6894\n",
            "Epoch: 005, AUC: 0.6487, AP: 0.6908\n",
            "Epoch: 006, AUC: 0.6498, AP: 0.6924\n",
            "Epoch: 007, AUC: 0.6509, AP: 0.6942\n",
            "Epoch: 008, AUC: 0.6514, AP: 0.6954\n",
            "Epoch: 009, AUC: 0.6519, AP: 0.6970\n",
            "Epoch: 010, AUC: 0.6526, AP: 0.6991\n",
            "Epoch: 011, AUC: 0.6525, AP: 0.7013\n",
            "Epoch: 012, AUC: 0.6523, AP: 0.7039\n",
            "Epoch: 013, AUC: 0.6514, AP: 0.7060\n",
            "Epoch: 014, AUC: 0.6507, AP: 0.7081\n",
            "Epoch: 015, AUC: 0.6500, AP: 0.7098\n",
            "Epoch: 016, AUC: 0.6485, AP: 0.7105\n",
            "Epoch: 017, AUC: 0.6473, AP: 0.7111\n",
            "Epoch: 018, AUC: 0.6461, AP: 0.7112\n",
            "Epoch: 019, AUC: 0.6459, AP: 0.7119\n",
            "Epoch: 020, AUC: 0.6457, AP: 0.7121\n",
            "Epoch: 021, AUC: 0.6454, AP: 0.7124\n",
            "Epoch: 022, AUC: 0.6455, AP: 0.7127\n",
            "Epoch: 023, AUC: 0.6456, AP: 0.7130\n",
            "Epoch: 024, AUC: 0.6455, AP: 0.7132\n",
            "Epoch: 025, AUC: 0.6460, AP: 0.7139\n",
            "Epoch: 026, AUC: 0.6466, AP: 0.7148\n",
            "Epoch: 027, AUC: 0.6472, AP: 0.7157\n",
            "Epoch: 028, AUC: 0.6489, AP: 0.7167\n",
            "Epoch: 029, AUC: 0.6514, AP: 0.7177\n",
            "Epoch: 030, AUC: 0.6555, AP: 0.7194\n",
            "Epoch: 031, AUC: 0.6610, AP: 0.7214\n",
            "Epoch: 032, AUC: 0.6671, AP: 0.7237\n",
            "Epoch: 033, AUC: 0.6719, AP: 0.7256\n",
            "Epoch: 034, AUC: 0.6761, AP: 0.7273\n",
            "Epoch: 035, AUC: 0.6807, AP: 0.7291\n",
            "Epoch: 036, AUC: 0.6854, AP: 0.7310\n",
            "Epoch: 037, AUC: 0.6921, AP: 0.7338\n",
            "Epoch: 038, AUC: 0.6995, AP: 0.7370\n",
            "Epoch: 039, AUC: 0.7057, AP: 0.7399\n",
            "Epoch: 040, AUC: 0.7134, AP: 0.7436\n",
            "Epoch: 041, AUC: 0.7217, AP: 0.7475\n",
            "Epoch: 042, AUC: 0.7299, AP: 0.7517\n",
            "Epoch: 043, AUC: 0.7357, AP: 0.7548\n",
            "Epoch: 044, AUC: 0.7399, AP: 0.7574\n",
            "Epoch: 045, AUC: 0.7420, AP: 0.7589\n",
            "Epoch: 046, AUC: 0.7426, AP: 0.7594\n",
            "Epoch: 047, AUC: 0.7443, AP: 0.7607\n",
            "Epoch: 048, AUC: 0.7469, AP: 0.7626\n",
            "Epoch: 049, AUC: 0.7500, AP: 0.7650\n",
            "Epoch: 050, AUC: 0.7536, AP: 0.7680\n",
            "Epoch: 051, AUC: 0.7564, AP: 0.7698\n",
            "Epoch: 052, AUC: 0.7573, AP: 0.7702\n",
            "Epoch: 053, AUC: 0.7578, AP: 0.7706\n",
            "Epoch: 054, AUC: 0.7591, AP: 0.7715\n",
            "Epoch: 055, AUC: 0.7602, AP: 0.7727\n",
            "Epoch: 056, AUC: 0.7607, AP: 0.7729\n",
            "Epoch: 057, AUC: 0.7615, AP: 0.7732\n",
            "Epoch: 058, AUC: 0.7621, AP: 0.7741\n",
            "Epoch: 059, AUC: 0.7624, AP: 0.7747\n",
            "Epoch: 060, AUC: 0.7628, AP: 0.7755\n",
            "Epoch: 061, AUC: 0.7640, AP: 0.7770\n",
            "Epoch: 062, AUC: 0.7649, AP: 0.7776\n",
            "Epoch: 063, AUC: 0.7654, AP: 0.7779\n",
            "Epoch: 064, AUC: 0.7656, AP: 0.7777\n",
            "Epoch: 065, AUC: 0.7653, AP: 0.7774\n",
            "Epoch: 066, AUC: 0.7640, AP: 0.7759\n",
            "Epoch: 067, AUC: 0.7642, AP: 0.7761\n",
            "Epoch: 068, AUC: 0.7631, AP: 0.7744\n",
            "Epoch: 069, AUC: 0.7630, AP: 0.7744\n",
            "Epoch: 070, AUC: 0.7635, AP: 0.7751\n",
            "Epoch: 071, AUC: 0.7638, AP: 0.7757\n",
            "Epoch: 072, AUC: 0.7637, AP: 0.7754\n",
            "Epoch: 073, AUC: 0.7638, AP: 0.7757\n",
            "Epoch: 074, AUC: 0.7625, AP: 0.7746\n",
            "Epoch: 075, AUC: 0.7622, AP: 0.7747\n",
            "Epoch: 076, AUC: 0.7627, AP: 0.7750\n",
            "Epoch: 077, AUC: 0.7635, AP: 0.7752\n",
            "Epoch: 078, AUC: 0.7636, AP: 0.7754\n",
            "Epoch: 079, AUC: 0.7634, AP: 0.7756\n",
            "Epoch: 080, AUC: 0.7632, AP: 0.7756\n",
            "Epoch: 081, AUC: 0.7622, AP: 0.7743\n",
            "Epoch: 082, AUC: 0.7613, AP: 0.7740\n",
            "Epoch: 083, AUC: 0.7613, AP: 0.7740\n",
            "Epoch: 084, AUC: 0.7620, AP: 0.7742\n",
            "Epoch: 085, AUC: 0.7634, AP: 0.7750\n",
            "Epoch: 086, AUC: 0.7649, AP: 0.7765\n",
            "Epoch: 087, AUC: 0.7653, AP: 0.7769\n",
            "Epoch: 088, AUC: 0.7655, AP: 0.7770\n",
            "Epoch: 089, AUC: 0.7648, AP: 0.7765\n",
            "Epoch: 090, AUC: 0.7650, AP: 0.7766\n",
            "Epoch: 091, AUC: 0.7659, AP: 0.7776\n",
            "Epoch: 092, AUC: 0.7667, AP: 0.7779\n",
            "Epoch: 093, AUC: 0.7668, AP: 0.7781\n",
            "Epoch: 094, AUC: 0.7660, AP: 0.7771\n",
            "Epoch: 095, AUC: 0.7659, AP: 0.7775\n",
            "Epoch: 096, AUC: 0.7671, AP: 0.7788\n",
            "Epoch: 097, AUC: 0.7687, AP: 0.7801\n",
            "Epoch: 098, AUC: 0.7702, AP: 0.7817\n",
            "Epoch: 099, AUC: 0.7709, AP: 0.7821\n",
            "Epoch: 100, AUC: 0.7704, AP: 0.7818\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "    loss = train()\n",
        "\n",
        "    auc, ap = test(data.test_pos_edge_index, data.test_neg_edge_index)\n",
        "    print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3pOB7hZZPEz",
        "outputId": "e524fe7c-7ccb-4847-9766-13a7c9a9970b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.3415,  0.3505],\n",
              "        [ 0.8631, -1.1042],\n",
              "        [-0.7020,  0.8189],\n",
              "        ...,\n",
              "        [ 0.0874, -0.0409],\n",
              "        [-0.6144,  0.7314],\n",
              "        [-0.6832,  0.7997]], device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Z = model.encode(x, train_pos_edge_index)\n",
        "Z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFyfH7ePZPEz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQpQ5Y40ZPEz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEkIqIWlZPEz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_AbD8YDZPEz"
      },
      "source": [
        "## Are the results (AUC) and (AP) easy to read and compare?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AtUACx2ZPEz"
      },
      "source": [
        "# Use Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UE1wU8KkZPEz"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "yAYjrMXhZPEz"
      },
      "outputs": [],
      "source": [
        "# parameters\n",
        "out_channels = 2\n",
        "num_features = dataset.num_features\n",
        "epochs = 100\n",
        "\n",
        "# model\n",
        "model = GAE(GCNEncoder(num_features, out_channels))\n",
        "\n",
        "# move to GPU (if available)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "x = data.x.to(device)\n",
        "train_pos_edge_index = data.train_pos_edge_index.to(device)\n",
        "\n",
        "# inizialize the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFwMnVhsZPEz"
      },
      "source": [
        "### Import tensorboard\n",
        "\n",
        "#### Installation: (if needed) \"pip install tensorboard\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ivWJSEdXZPE0"
      },
      "outputs": [],
      "source": [
        "writer = SummaryWriter('runs/GAE1_experiment_'+'2d_100_epochs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "scrolled": true,
        "id": "roSgKW_rZPE0",
        "outputId": "b840e76f-6883-4e0f-e274-f45e0be6a6a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, AUC: 0.6301, AP: 0.6722\n",
            "Epoch: 002, AUC: 0.6491, AP: 0.6865\n",
            "Epoch: 003, AUC: 0.6535, AP: 0.6906\n",
            "Epoch: 004, AUC: 0.6552, AP: 0.6921\n",
            "Epoch: 005, AUC: 0.6560, AP: 0.6935\n",
            "Epoch: 006, AUC: 0.6564, AP: 0.6944\n",
            "Epoch: 007, AUC: 0.6570, AP: 0.6955\n",
            "Epoch: 008, AUC: 0.6573, AP: 0.6964\n",
            "Epoch: 009, AUC: 0.6575, AP: 0.6978\n",
            "Epoch: 010, AUC: 0.6579, AP: 0.6994\n",
            "Epoch: 011, AUC: 0.6587, AP: 0.7018\n",
            "Epoch: 012, AUC: 0.6589, AP: 0.7041\n",
            "Epoch: 013, AUC: 0.6592, AP: 0.7070\n",
            "Epoch: 014, AUC: 0.6593, AP: 0.7095\n",
            "Epoch: 015, AUC: 0.6589, AP: 0.7116\n",
            "Epoch: 016, AUC: 0.6584, AP: 0.7136\n",
            "Epoch: 017, AUC: 0.6577, AP: 0.7152\n",
            "Epoch: 018, AUC: 0.6565, AP: 0.7160\n",
            "Epoch: 019, AUC: 0.6560, AP: 0.7170\n",
            "Epoch: 020, AUC: 0.6553, AP: 0.7171\n",
            "Epoch: 021, AUC: 0.6543, AP: 0.7170\n",
            "Epoch: 022, AUC: 0.6537, AP: 0.7170\n",
            "Epoch: 023, AUC: 0.6534, AP: 0.7170\n",
            "Epoch: 024, AUC: 0.6529, AP: 0.7170\n",
            "Epoch: 025, AUC: 0.6532, AP: 0.7177\n",
            "Epoch: 026, AUC: 0.6532, AP: 0.7183\n",
            "Epoch: 027, AUC: 0.6532, AP: 0.7189\n",
            "Epoch: 028, AUC: 0.6534, AP: 0.7194\n",
            "Epoch: 029, AUC: 0.6542, AP: 0.7203\n",
            "Epoch: 030, AUC: 0.6559, AP: 0.7219\n",
            "Epoch: 031, AUC: 0.6586, AP: 0.7233\n",
            "Epoch: 032, AUC: 0.6626, AP: 0.7247\n",
            "Epoch: 033, AUC: 0.6670, AP: 0.7262\n",
            "Epoch: 034, AUC: 0.6723, AP: 0.7283\n",
            "Epoch: 035, AUC: 0.6776, AP: 0.7303\n",
            "Epoch: 036, AUC: 0.6821, AP: 0.7321\n",
            "Epoch: 037, AUC: 0.6862, AP: 0.7337\n",
            "Epoch: 038, AUC: 0.6909, AP: 0.7357\n",
            "Epoch: 039, AUC: 0.6972, AP: 0.7383\n",
            "Epoch: 040, AUC: 0.7052, AP: 0.7417\n",
            "Epoch: 041, AUC: 0.7148, AP: 0.7465\n",
            "Epoch: 042, AUC: 0.7232, AP: 0.7505\n",
            "Epoch: 043, AUC: 0.7282, AP: 0.7530\n",
            "Epoch: 044, AUC: 0.7317, AP: 0.7551\n",
            "Epoch: 045, AUC: 0.7337, AP: 0.7561\n",
            "Epoch: 046, AUC: 0.7359, AP: 0.7577\n",
            "Epoch: 047, AUC: 0.7382, AP: 0.7588\n",
            "Epoch: 048, AUC: 0.7412, AP: 0.7607\n",
            "Epoch: 049, AUC: 0.7450, AP: 0.7632\n",
            "Epoch: 050, AUC: 0.7477, AP: 0.7650\n",
            "Epoch: 051, AUC: 0.7510, AP: 0.7668\n",
            "Epoch: 052, AUC: 0.7532, AP: 0.7680\n",
            "Epoch: 053, AUC: 0.7547, AP: 0.7688\n",
            "Epoch: 054, AUC: 0.7555, AP: 0.7692\n",
            "Epoch: 055, AUC: 0.7564, AP: 0.7699\n",
            "Epoch: 056, AUC: 0.7575, AP: 0.7709\n",
            "Epoch: 057, AUC: 0.7581, AP: 0.7713\n",
            "Epoch: 058, AUC: 0.7583, AP: 0.7716\n",
            "Epoch: 059, AUC: 0.7589, AP: 0.7723\n",
            "Epoch: 060, AUC: 0.7584, AP: 0.7720\n",
            "Epoch: 061, AUC: 0.7582, AP: 0.7721\n",
            "Epoch: 062, AUC: 0.7584, AP: 0.7723\n",
            "Epoch: 063, AUC: 0.7586, AP: 0.7718\n",
            "Epoch: 064, AUC: 0.7585, AP: 0.7717\n",
            "Epoch: 065, AUC: 0.7589, AP: 0.7722\n",
            "Epoch: 066, AUC: 0.7592, AP: 0.7723\n",
            "Epoch: 067, AUC: 0.7598, AP: 0.7730\n",
            "Epoch: 068, AUC: 0.7601, AP: 0.7731\n",
            "Epoch: 069, AUC: 0.7600, AP: 0.7729\n",
            "Epoch: 070, AUC: 0.7598, AP: 0.7726\n",
            "Epoch: 071, AUC: 0.7601, AP: 0.7730\n",
            "Epoch: 072, AUC: 0.7609, AP: 0.7737\n",
            "Epoch: 073, AUC: 0.7611, AP: 0.7738\n",
            "Epoch: 074, AUC: 0.7606, AP: 0.7732\n",
            "Epoch: 075, AUC: 0.7598, AP: 0.7720\n",
            "Epoch: 076, AUC: 0.7593, AP: 0.7716\n",
            "Epoch: 077, AUC: 0.7592, AP: 0.7713\n",
            "Epoch: 078, AUC: 0.7593, AP: 0.7709\n",
            "Epoch: 079, AUC: 0.7609, AP: 0.7730\n",
            "Epoch: 080, AUC: 0.7618, AP: 0.7739\n",
            "Epoch: 081, AUC: 0.7627, AP: 0.7751\n",
            "Epoch: 082, AUC: 0.7634, AP: 0.7760\n",
            "Epoch: 083, AUC: 0.7631, AP: 0.7753\n",
            "Epoch: 084, AUC: 0.7635, AP: 0.7755\n",
            "Epoch: 085, AUC: 0.7641, AP: 0.7757\n",
            "Epoch: 086, AUC: 0.7655, AP: 0.7770\n",
            "Epoch: 087, AUC: 0.7670, AP: 0.7787\n",
            "Epoch: 088, AUC: 0.7678, AP: 0.7793\n",
            "Epoch: 089, AUC: 0.7686, AP: 0.7802\n",
            "Epoch: 090, AUC: 0.7688, AP: 0.7798\n",
            "Epoch: 091, AUC: 0.7690, AP: 0.7801\n",
            "Epoch: 092, AUC: 0.7687, AP: 0.7788\n",
            "Epoch: 093, AUC: 0.7701, AP: 0.7805\n",
            "Epoch: 094, AUC: 0.7723, AP: 0.7825\n",
            "Epoch: 095, AUC: 0.7737, AP: 0.7835\n",
            "Epoch: 096, AUC: 0.7731, AP: 0.7829\n",
            "Epoch: 097, AUC: 0.7728, AP: 0.7830\n",
            "Epoch: 098, AUC: 0.7722, AP: 0.7819\n",
            "Epoch: 099, AUC: 0.7728, AP: 0.7824\n",
            "Epoch: 100, AUC: 0.7745, AP: 0.7843\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "    loss = train()\n",
        "    auc, ap = test(data.test_pos_edge_index, data.test_neg_edge_index)\n",
        "    print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))\n",
        "\n",
        "\n",
        "    writer.add_scalar('auc train',auc,epoch) # new line\n",
        "    writer.add_scalar('ap train',ap,epoch)   # new line"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wKEMDnPdcxP0",
        "outputId": "c079417c-74d6-4c82-c026-2e03688ae5f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoSLsFY6ZPE0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDxUH4dxZPE0"
      },
      "source": [
        "## Graph Variational AutoEncoder (GVAE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOdn5wIxZPE0"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import VGAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBD08ERsZPE0",
        "outputId": "e189953d-faa4-4790-ca51-488b1127e5c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/antonio/anaconda3/envs/geometric_new/lib/python3.9/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ],
      "source": [
        "dataset = Planetoid(\"\\..\", \"CiteSeer\", transform=T.NormalizeFeatures())\n",
        "data = dataset[0]\n",
        "data.train_mask = data.val_mask = data.test_mask = data.y = None\n",
        "data = train_test_split_edges(data)\n",
        "\n",
        "\n",
        "class VariationalGCNEncoder(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(VariationalGCNEncoder, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 2 * out_channels, cached=True) # cached only for transductive learning\n",
        "        self.conv_mu = GCNConv(2 * out_channels, out_channels, cached=True)\n",
        "        self.conv_logstd = GCNConv(2 * out_channels, out_channels, cached=True)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lz9xhVXZPE0"
      },
      "outputs": [],
      "source": [
        "out_channels = 2\n",
        "num_features = dataset.num_features\n",
        "epochs = 300\n",
        "\n",
        "\n",
        "model = VGAE(VariationalGCNEncoder(num_features, out_channels))  # new line\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "x = data.x.to(device)\n",
        "train_pos_edge_index = data.train_pos_edge_index.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyxHzhxPZPE0"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    z = model.encode(x, train_pos_edge_index)\n",
        "    loss = model.recon_loss(z, train_pos_edge_index)\n",
        "\n",
        "    loss = loss + (1 / data.num_nodes) * model.kl_loss()  # new line\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return float(loss)\n",
        "\n",
        "\n",
        "def test(pos_edge_index, neg_edge_index):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        z = model.encode(x, train_pos_edge_index)\n",
        "    return model.test(z, pos_edge_index, neg_edge_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "F04y1jJNZPE0",
        "outputId": "e2ca95cd-00ea-4dd3-c306-8f3633f92809"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 001, AUC: 0.6043, AP: 0.6264\n",
            "Epoch: 002, AUC: 0.4937, AP: 0.5243\n",
            "Epoch: 003, AUC: 0.4943, AP: 0.5001\n",
            "Epoch: 004, AUC: 0.5013, AP: 0.5079\n",
            "Epoch: 005, AUC: 0.5011, AP: 0.5006\n",
            "Epoch: 006, AUC: 0.5022, AP: 0.5011\n",
            "Epoch: 007, AUC: 0.5022, AP: 0.5011\n",
            "Epoch: 008, AUC: 0.5022, AP: 0.5011\n",
            "Epoch: 009, AUC: 0.4978, AP: 0.5007\n",
            "Epoch: 010, AUC: 0.4978, AP: 0.5007\n",
            "Epoch: 011, AUC: 0.4978, AP: 0.5007\n",
            "Epoch: 012, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 013, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 014, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 015, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 016, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 017, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 018, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 019, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 020, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 021, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 022, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 023, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 024, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 025, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 026, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 027, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 028, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 029, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 030, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 031, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 032, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 033, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 034, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 035, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 036, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 037, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 038, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 039, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 040, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 041, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 042, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 043, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 044, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 045, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 046, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 047, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 048, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 049, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 050, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 051, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 052, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 053, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 054, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 055, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 056, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 057, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 058, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 059, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 060, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 061, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 062, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 063, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 064, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 065, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 066, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 067, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 068, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 069, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 070, AUC: 0.5000, AP: 0.5000\n",
            "Epoch: 071, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 072, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 073, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 074, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 075, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 076, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 077, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 078, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 079, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 080, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 081, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 082, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 083, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 084, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 085, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 086, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 087, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 088, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 089, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 090, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 091, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 092, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 093, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 094, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 095, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 096, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 097, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 098, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 099, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 100, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 101, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 102, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 103, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 104, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 105, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 106, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 107, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 108, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 109, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 110, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 111, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 112, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 113, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 114, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 115, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 116, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 117, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 118, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 119, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 120, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 121, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 122, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 123, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 124, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 125, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 126, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 127, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 128, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 129, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 130, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 131, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 132, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 133, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 134, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 135, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 136, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 137, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 138, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 139, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 140, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 141, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 142, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 143, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 144, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 145, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 146, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 147, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 148, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 149, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 150, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 151, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 152, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 153, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 154, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 155, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 156, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 157, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 158, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 159, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 160, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 161, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 162, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 163, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 164, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 165, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 166, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 167, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 168, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 169, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 170, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 171, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 172, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 173, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 174, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 175, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 176, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 177, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 178, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 179, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 180, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 181, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 182, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 183, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 184, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 185, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 186, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 187, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 188, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 189, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 190, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 191, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 192, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 193, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 194, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 195, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 196, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 197, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 198, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 199, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 200, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 201, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 202, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 203, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 204, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 205, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 206, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 207, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 208, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 209, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 210, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 211, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 212, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 213, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 214, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 215, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 216, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 217, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 218, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 219, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 220, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 221, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 222, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 223, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 224, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 225, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 226, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 227, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 228, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 229, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 230, AUC: 0.4989, AP: 0.4995\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 231, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 232, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 233, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 234, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 235, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 236, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 237, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 238, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 239, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 240, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 241, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 242, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 243, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 244, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 245, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 246, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 247, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 248, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 249, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 250, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 251, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 252, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 253, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 254, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 255, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 256, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 257, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 258, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 259, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 260, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 261, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 262, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 263, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 264, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 265, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 266, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 267, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 268, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 269, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 270, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 271, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 272, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 273, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 274, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 275, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 276, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 277, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 278, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 279, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 280, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 281, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 282, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 283, AUC: 0.4989, AP: 0.4995\n",
            "Epoch: 284, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 285, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 286, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 287, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 288, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 289, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 290, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 291, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 292, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 293, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 294, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 295, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 296, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 297, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 298, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 299, AUC: 0.5011, AP: 0.5011\n",
            "Epoch: 300, AUC: 0.5011, AP: 0.5011\n"
          ]
        }
      ],
      "source": [
        "writer = SummaryWriter('runs/VGAE_experiment_'+'2d_100_epochs')\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    loss = train()\n",
        "    auc, ap = test(data.test_pos_edge_index, data.test_neg_edge_index)\n",
        "    print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))\n",
        "\n",
        "\n",
        "    writer.add_scalar('auc train',auc,epoch) # new line\n",
        "    writer.add_scalar('ap train',ap,epoch)   # new line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYFfRG7iZPE0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}